{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVh2qdLq_p1N"
      },
      "source": [
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "¬†¬†¬† Artificial Intelligence <br>\n",
        "<font color=2565AE size=5>\n",
        "¬†¬†¬† Computer Engineering Department <br>\n",
        "¬†¬†¬† Spring 2025<br>\n",
        "<font color=3C99D size=5>\n",
        "    Project-Phase2<br>\n",
        "    Soft Actor Critic<br>\n",
        "<font color=696880 size=4>\n",
        "¬†¬†¬† Ali Najar-Mohmmad Shafizade-Armin Khosravi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxu_TL6Ohi2H"
      },
      "source": [
        "\n",
        "\n",
        "In this notebook, we are going to get familiar with SAC algorithm. Soft Actor Critic (SAC) is an off-policy algorithm that maximizes a combination of expected return **and** entropy. Higher entropy results in higher exploration, which is an important concept in Reinforcement Learning.\n",
        "\n",
        "## üì¶ Setup and Dependencies\n",
        "\n",
        "Install PyBullet for Physics based environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdCnFKtwcc6N",
        "outputId": "03b10c6e-d457-436e-c225-5318b7803216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m80.5/80.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Tue Sep  2 20:43:56 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pybullet Box2D\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftc5RDuQ7B8T"
      },
      "source": [
        "Import necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lsOeW66nhuiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "763c5e0d-8693-46b8-ccf7-768a28f36500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from gym.wrappers import RecordVideo\n",
        "import gym\n",
        "import pybullet_envs\n",
        "np.bool8 = np.bool_\n",
        "from tqdm.notebook import trange\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrFtqt8Pwc23"
      },
      "source": [
        "## üìà Utility codes\n",
        "\n",
        "We will use this utility function to visualize the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Gjp1OnqxcBN"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(x, filename, save_plot=True):\n",
        "    avg_x = [np.mean(x[np.max([0, i - 100]):i]) for i in range(len(x))]\n",
        "    plt.figure(dpi=200)\n",
        "    plt.title('Learning Curve')\n",
        "    plt.plot(range(len(x)), x, label='score', alpha=0.3)\n",
        "    plt.plot(range(len(avg_x)), avg_x, label='average score')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    if save_plot:\n",
        "        plt.savefig(filename + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pfLBe7P-DZw"
      },
      "source": [
        "This class implements a **Replay Buffer** to store and sample transitions of the form $(s_t, a_t, r_t, s_{t+1}, d_t)$ to break correlation in updates for stability in mini-batch stochastic gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "peFGke2lw_fs"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, state_dims, action_dims):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.ptr = 0\n",
        "        self.is_full = False\n",
        "\n",
        "\n",
        "        # TODO: Initialize buffer arrays to store states, next states, actions, rewards, and done flags\n",
        "        self.states = np.zeros((buffer_size, state_dims), dtype=np.float32)\n",
        "        self.next_states = np.zeros((buffer_size, state_dims), dtype=np.float32)\n",
        "        self.actions = np.zeros((buffer_size, action_dims), dtype=np.float32)\n",
        "        self.rewards = np.zeros((buffer_size, 1), dtype=np.float32)\n",
        "        self.dones = np.zeros((buffer_size, 1), dtype=np.float32)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        # TODO : Store the transition in the replay buffer\n",
        "        self.states[self.ptr] = state\n",
        "        self.actions[self.ptr] = action\n",
        "        self.rewards[self.ptr] = reward\n",
        "        self.next_states[self.ptr] = state_\n",
        "        self.dones[self.ptr] = done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "        if self.ptr == 0:\n",
        "            self.is_full = True\n",
        "\n",
        "\n",
        "    def load_batch(self, batch_size):\n",
        "        # TODO: Sample a random batch of transitions from the buffer\n",
        "\n",
        "        max_mem = self.buffer_size if self.is_full else self.ptr\n",
        "        batch_indices = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.states[batch_indices]\n",
        "        actions = self.actions[batch_indices]\n",
        "        rewards = self.rewards[batch_indices]\n",
        "        next_states = self.next_states[batch_indices]\n",
        "        dones = self.dones[batch_indices]\n",
        "\n",
        "        return states, actions, rewards, states_, done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D93nKTCK-l3p"
      },
      "source": [
        "## üß± Neural Networks\n",
        "\n",
        "This cell defines three core neural networks used in SAC:\n",
        "\n",
        "- **Critic Network:**\n",
        "Estimates the **Q-value function** $ Q(s, a) $. Two critics are used to mitigate overestimation bias.\n",
        "\n",
        "- **Value Network:**\n",
        "Estimates the **state value function** $ V(s) $, used to train the actor and as a baseline.\n",
        "\n",
        "- **Actor Network:**\n",
        "Outputs the **mean** and **standard deviation** for a Gaussian policy\n",
        "$\n",
        "\\pi(a|s) = \\mathcal{N}(\\mu(s), \\sigma(s))\n",
        "$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLEiyCTLxLNS"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, beta, state_dims, action_dims, fc1_dims, fc2_dims, name='Critic', ckpt_dir='tmp'):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # TODO 1: Save input arguments as attributes\n",
        "        self.input_dims = state_dims\n",
        "        self.action_dims = action_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.checkpoint_dir = ckpt_dir\n",
        "        self.checkpoint_file = os.path.join(ckpt_dir, name + '_sac')\n",
        "\n",
        "        # Device (GPU if available, else CPU)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # TODO 2: Define the first fully connected layer\n",
        "        # The input is the concatenation of state and action\n",
        "        self.fc1 = nn.Linear(self.input_dims + self.action_dims, self.fc1_dims)\n",
        "\n",
        "        # TODO 3: Define the second hidden layer and the final Q-value output layer\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.q = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "\n",
        "        # TODO 4: Define the optimizer. Assign to the correct device\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        return q\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.ckpt_path)\n",
        "\n",
        "    def load_checkpoint(self, gpu_to_cpu=False):\n",
        "        if gpu_to_cpu:\n",
        "            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n",
        "        else:\n",
        "            self.load_state_dict(T.load(self.ckpt_path))\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, alpha, state_dims, action_dims, fc1_dims, fc2_dims, max_action, reparam_noise,\n",
        "                 name='Actor', ckpt_dir='tmp'):\n",
        "        super(Actor, self).__init__()\n",
        "        # TODO 1: Store initialization parameters\n",
        "        self.state_dims = state_dims\n",
        "        self.action_dims = action_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.max_action = max_action\n",
        "        self.reparam_noise = reparam_noise\n",
        "        self.checkpoint_dir = ckpt_dir\n",
        "        self.ckpt_path = os.path.join(ckpt_dir, name + '_sac')\n",
        "\n",
        "        # Device\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "        # TODO 2: Define fully connected layers to transform input state\n",
        "        self.fc1 = nn.Linear(self.state_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "\n",
        "        # TODO 3: Output mean and standard deviation for the Gaussian policy\n",
        "        self.mu = nn.Linear(self.fc2_dims, self.action_dims)\n",
        "        self.sigma = nn.Linear(self.fc2_dims, self.action_dims)\n",
        "\n",
        "        # TODO 4: Define optimizer and move model to the appropriate device\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        #TODO\n",
        "        # Forward pass through shared layers\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        mu = self.mu(x)\n",
        "        sigma = self.sigma(x)\n",
        "\n",
        "        # Clamp sigma for numerical stability\n",
        "        sigma = T.clamp(sigma, min=-20, max=2)  # log std range\n",
        "\n",
        "        return mu, sigma\n",
        "\n",
        "    def sample_normal(self, state, reparameterize=True):\n",
        "        # TODO 7: Use the actor to compute distribution parameters\n",
        "        mu, sigma = self.forward(state)\n",
        "        sigma =  sigma.exp()\n",
        "\n",
        "        # TODO 8: Create a normal distribution and sample from it\n",
        "        dist = Normal(mu, sigma)\n",
        "\n",
        "        if reparameterize:\n",
        "            actions = dist.rsample()\n",
        "        else:\n",
        "            actions = dist.sample()\n",
        "\n",
        "       # Apply tanh squashing and rescale to action space\n",
        "        action = T.tanh(actions) * self.max_action\n",
        "\n",
        "        # Compute log probabilities (with tanh correction)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        log_probs -= T.log(1 - T.tanh(actions).pow(2) + 1e-6)\n",
        "        log_probs = log_probs.sum(1, keepdim=True)\n",
        "\n",
        "        return action, log_probs\n",
        "\n",
        "\n",
        "        return action, log_probs\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.ckpt_path)\n",
        "\n",
        "    def load_checkpoint(self, gpu_to_cpu=False):\n",
        "        if gpu_to_cpu:\n",
        "            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n",
        "        else:\n",
        "            self.load_state_dict(T.load(self.ckpt_path))\n",
        "\n",
        "\n",
        "class Value(nn.Module):\n",
        "    def __init__(self, beta, state_dims, fc1_dims, fc2_dims, name='Value', ckpt_dir='tmp'):\n",
        "        super(Value, self).__init__()\n",
        "\n",
        "        # TODO 1: Save arguments as instance variables\n",
        "\n",
        "        self.input_dims = state_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.checkpoint_dir = ckpt_dir\n",
        "        self.checkpoint_file = os.path.join(ckpt_dir, name + '_sac')\n",
        "\n",
        "        # Device\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # TODO 2: Define the fully connected layers for value approximation\n",
        "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.v = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # TODO 3: Set optimizer and device\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        v = self.v(x)\n",
        "        return v\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.ckpt_path)\n",
        "\n",
        "    def load_checkpoint(self, gpu_to_cpu=False):\n",
        "        if gpu_to_cpu:\n",
        "            self.load_state_dict(T.load(self.ckpt_path, map_location=lambda storage, loc: storage))\n",
        "        else:\n",
        "            self.load_state_dict(T.load(self.ckpt_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM9_xaEl_7Cj"
      },
      "source": [
        "## ü§ñ Agent Implementation\n",
        "\n",
        "This class encapsulates the full logic of the SAC agent. In general, the learning process uses entropy-regularized policy gradients $J_\\pi = \\mathbb{E}_{s_t \\sim D, a_t \\sim \\pi} \\left[ \\alpha \\log(\\pi(a_t|s_t)) - Q(s_t, a_t) \\right]$ with soft target updates $\\theta_{\\text{target}} \\leftarrow \\tau \\theta + (1 - \\tau)\\theta_{\\text{target}}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO2Gnvlfw7up"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, gamma, alpha, beta, state_dims, action_dims, max_action, fc1_dim, fc2_dim,\n",
        "                 memory_size, batch_size, tau, update_period, reward_scale, warmup, reparam_noise_lim,\n",
        "                 name, ckpt_dir='tmp'):\n",
        "        # TODO 1: Save all hyperparameters and paths\n",
        "\n",
        "\n",
        "\n",
        "        model_name = f'{name}__' \\\n",
        "                     f'gamma_{gamma}__' \\\n",
        "                     f'alpha_{alpha}__' \\\n",
        "                     f'beta_{beta}__' \\\n",
        "                     f'fc1_{fc1_dim}__' \\\n",
        "                     f'fc2_{fc2_dim}__' \\\n",
        "                     f'bs_{batch_size}__' \\\n",
        "                     f'buffer_{memory_size}__' \\\n",
        "                     f'update_period_{update_period}__' \\\n",
        "                     f'tau_{tau}__'\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.learn_iter = 0\n",
        "        self.full_path = os.path.join(self.ckpt_dir, self.model_name)\n",
        "\n",
        "        # TODO 2: Initialize the replay buffer\n",
        "\n",
        "\n",
        "\n",
        "        # TODO 3: Initialize Actor and Critic networks\n",
        "\n",
        "\n",
        "\n",
        "        # TODO 4: Initialize Value and Target Value networks\n",
        "\n",
        "\n",
        "\n",
        "        # TODO 5: Sync the parameters of value and target_value networks initially\n",
        "\n",
        "    def choose_action(self, state, deterministic=False, reparameterize=False):\n",
        "\n",
        "        # TODO 6: Convert state to tensor, move to device, and add batch dimension\n",
        "\n",
        "        # TODO 7: Choose action from actor\n",
        "\n",
        "        pass\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        pass\n",
        "\n",
        "    def load_batch(self):\n",
        "\n",
        "        return states, actions, rewards, states_, done\n",
        "\n",
        "    def update_parameters(self, tau=None):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic_1.save_checkpoint()\n",
        "        self.critic_2.save_checkpoint()\n",
        "        self.value.save_checkpoint()\n",
        "        self.target_value.save_checkpoint()\n",
        "\n",
        "    def load_model(self, gpu_to_cpu=False):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.actor.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n",
        "        self.critic_1.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n",
        "        self.critic_2.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n",
        "        self.value.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n",
        "        self.target_value.load_checkpoint(gpu_to_cpu=gpu_to_cpu)\n",
        "\n",
        "    def learn(self):\n",
        "\n",
        "        # TODO 11: Skip learning during warm-up period or insufficient samples\n",
        "\n",
        "\n",
        "        # === VALUE LOSS ===\n",
        "        # TODO 12: Load batch and sample action\n",
        "\n",
        "\n",
        "        # TODO 13: Estimate Q-values\n",
        "\n",
        "\n",
        "        # TODO 14: Compute target value\n",
        "\n",
        "        # TODO 15: Compute value loss and update value network\n",
        "\n",
        "        # === ACTOR LOSS ===\n",
        "        # TODO 16: Re-sample actions (this time with reparameterization for gradients)\n",
        "\n",
        "        # TODO 17: maximize entropy-regularized Q-value\n",
        "\n",
        "        # === CRITIC LOSS ===\n",
        "        # TODO 18: Compute Q targets\n",
        "\n",
        "        # TODO 19: Compute MSE loss for both critics\n",
        "\n",
        "        # === TARGET NETWORK UPDATE ===\n",
        "\n",
        "        # TODO 21: Increase learning iteration counter\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjXrh27QBTkn"
      },
      "source": [
        "## ‚öôÔ∏è Training Configuration\n",
        "\n",
        "Set up your training parameters. `HalfCheetahBulletEnv-v0` is a continuous control task where the agent must learn to run using articulated legs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZaWTp5Ufczm"
      },
      "outputs": [],
      "source": [
        "# Configuration parameters\n",
        "\n",
        "\n",
        "env_name = 'HalfCheetahBulletEnv-v0'\n",
        "dir = 'tmp'\n",
        "n_games = None\n",
        "\n",
        "\n",
        "gamma = 0.99\n",
        "alpha = 3e-4\n",
        "beta = 3e-4\n",
        "fc1_dim = 256\n",
        "fc2_dim = 256\n",
        "# Add other network dims if needed\n",
        "memory_size = None\n",
        "batch_size = None\n",
        "tau = 0.005\n",
        "update_period = 2\n",
        "reward_scale = 2.\n",
        "warmup = None\n",
        "reparam_noise_lim = 1e-6\n",
        "record_video = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMysPzLMiJ5q"
      },
      "source": [
        "## üöÄ Training Loop\n",
        "\n",
        "For each episode, interact with the environment to collect transitions, then update the SAC networks and save the best model.\n",
        "\n",
        "After training, a learning curve is plotted to visualize convergence and performance stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMdDnnagfDxw"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name)\n",
        "dir_path = os.path.join(dir, env_name)\n",
        "os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "if record_video:\n",
        "    env = RecordVideo(env, video_folder=os.path.join(dir_path, 'videos'),\n",
        "                      episode_trigger=lambda ep: ep == n_games - 1)\n",
        "\n",
        "\n",
        "\n",
        "agent = None\n",
        "\n",
        "\n",
        "# TODO: Initialize performance tracking variables\n",
        "\n",
        "\n",
        "\n",
        "for game in trange(n_games):\n",
        "    # TODO: Reset environment and initialize variables at the start of each episode\n",
        "\n",
        "\n",
        "    # TODO: Interact with the environment until the episode is done\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Track score and average score for plotting and saving\n",
        "\n",
        "    print(f'| Game: {game:6.0f} | Score: {score:10.2f} | Best score: {best_score:10.2f} | '\n",
        "          f'Avg score {avg_score:10.2f} | Learning iter: {agent.learn_iter:10.0f} |')\n",
        "\n",
        "\n",
        "    # TODO: Save the model if the current avg score is better than the best so far\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "plot_learning_curve(scores, agent.full_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmPLjy9zC1QI"
      },
      "source": [
        "## üé• Visualize Agent Behavior\n",
        "\n",
        "This is the last episode recorded video of the trained agent interacting with the environment in training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkIwmYFSg8nB"
      },
      "outputs": [],
      "source": [
        "Video(f\"/content/tmp/HalfCheetahBulletEnv-v0/videos/rl-video-episode-{n_games-1}.mp4\", embed=True, width=600)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}